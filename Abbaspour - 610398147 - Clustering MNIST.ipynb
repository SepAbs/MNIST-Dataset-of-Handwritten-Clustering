{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbaspour - 610398147 - Clustering MNIST\n",
    "The problem is reduce feature of MNIST dataset images with an autoencoder becomes tuned and trained, and use encoded images (from three set train set, test set and validation set (35% of test set)) to get clustered through four clustering algorithms: K-Means, Mini-Batch K-Means, DBSCAN and HDBSCAN.\n",
    "Firrst some methods of some libraries are imported for calculations and plottings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clusteval import clusteval\n",
    "from hdbscan import HDBSCAN \n",
    "from hnet import enrichment\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD\n",
    "from matplotlib.pyplot import bar, colorbar, figure, gray, imshow, ion, legend, plot, savefig, scatter, show, style, subplot, subplots, title, xlabel, xticks, ylabel\n",
    "from numpy import argsort, array, cumsum, matmul, prod, random, reshape, round, sort, sqrt, unique, vstack, where\n",
    "from os import environ\n",
    "from pandas import read_csv\n",
    "from plotly.graph_objs import Figure, Layout, Scatter3d\n",
    "from plotly.offline import iplot\n",
    "from seaborn import histplot\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.cluster import DBSCAN, KMeans, MiniBatchKMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, adjusted_mutual_info_score, adjusted_rand_score, calinski_harabasz_score, completeness_score, confusion_matrix, ConfusionMatrixDisplay, davies_bouldin_score, homogeneity_score, mutual_info_score, pairwise_distances_argmin, silhouette_score, rand_score, v_measure_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "environ[\"KMP_DUPLICATE_LIB_OK\"], environ[\"TF_CPP_MIN_LOG_LEVEL\"], environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"TRUE\", \"3\", \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset & Defining Variables\n",
    "Train set and test set are read as data frames.\n",
    "As a preprocessing, values are divided by 255 as they're in range (0, 255) as a normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading & normalizing the dataset\n",
    "trainSet, testSet = read_csv(\"mnist_train.csv\"), read_csv(\"mnist_test.csv\")\n",
    "df = trainSet._append(testSet, ignore_index = True)\n",
    "X_train, X_test, y_train, y_test, Optimizers, Activations, Architectures, Initializers, learningRates, Epochs, batchSizes, Colors, lossFunction, Accuracy, Loss, validationAccuracy, validationLoss, Title, Alpha, localPopulation, Numbers, numberIterations, numberGenerations, Fitness, Parameters, Tuner = trainSet.drop([\"label\"], axis = 1).astype(\"float32\") / 255., testSet.drop([\"label\"], axis = 1).astype(\"float32\") / 255., trainSet[\"label\"].astype(\"int\"), testSet[\"label\"].astype(\"int\"), {Adam: \"Adam\" , Nadam: \"Nadam\", SGD: \"SGD\"}, [\"linear\", \"relu\", \"tanh\"], [(128, 16), (500, 100, 30), (128, 64, 32)], [\"glorot_normal\", \"glorot_uniform\", \"zero\"], [0.001, 0.01, 0.1, 0.2, 0.25, 0.3, 0.5, 0.8], [1, 10, 15, 25, 50], [256, 300, 500, 1000, 5000, 10000, 30000, 60000], [\"black\", \"blue\", \"cyan\", \"green\", \"magenta\", \"maroon\", \"purple\", \"red\", \"teal\", \"yellow\"], \"mean_squared_error\", \"accuracy\", \"loss\", \"val_accuracy\", \"val_loss\", \"Model Evaluation\", 0.6, 1, 10, 1, 1, [], [], {}\n",
    "dfX_train, optimizerNames, localBound, number_test_samples, Beta = X_train, list(Optimizers.values()), X_train.shape[1], len(X_test), 1 - Alpha\n",
    "\n",
    "# Defining the encoder input & decoder input dimentions.\n",
    "Image, Length, Start = Input(shape = (localBound,)), int(sqrt(localBound)), int(number_test_samples * 0.35)\n",
    "\n",
    "# 0.35% of test samples are chosen as validation samples\n",
    "validationSet = testSet.iloc[Start : number_test_samples - Start]\n",
    "y_val, X_val, arrX_train, arrX_test, arrtestSet = validationSet[\"label\"], validationSet.drop([\"label\"], axis = 1), X_train.to_numpy().reshape(-1, 28, 28), X_test.to_numpy().reshape(-1, 28, 28), testSet.to_numpy()\n",
    "print(f\"There're {len(X_train)} samples of {Length} X {Length} images as train samples and {number_test_samples} samples as test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Autoencoder\n",
    "Target is to minimizer Mean Square Error value as error function.\n",
    "Autoencoder is fine tuned by searching step by step between three optimizers 'Adam', 'Nadam' & 'SGD', three architectures (128, 16), (500, 100, 30) and (128, 64, 32), weight initializers methods 'Glorot Normal', 'Glorot Uniform' and 'Zero' and activation functions 'Linear', 'Relu' and 'Tanh'.\n",
    "When best optimizer is found by searching, model would be search for good learning rate between candidate learning rates.\n",
    "In tuning, batch size equals to 60000 which is the number of train samples and search for each tuple of hyperparameters comes done in 1 epoch due to time limitation and also there isn't sensibe differences between searching in different epochs but getting closer and closer to overfitting event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning hyperparameters\n",
    "for Optimizer in optimizerNames:\n",
    "    for Architecture in Architectures:\n",
    "        decoderInput, number_hidden_layers = Input(shape = (Architecture[-1],)), len(Architecture) - 1\n",
    "        for Initializer in Initializers:\n",
    "            for Activation in Activations:\n",
    "                # Define & build the encoder & decoder\n",
    "                Encoded, Decoded = Dense(Architecture[0], activation = Activation, kernel_initializer = Initializer)(Image), Dense(Architecture[-2], activation = Activation, kernel_initializer = Initializer)(decoderInput)\n",
    "                for hiddenLayer in range(1, number_hidden_layers):\n",
    "                    Encoded, Decoded = Dense(Architecture[hiddenLayer], activation = Activation, kernel_initializer = Initializer)(Encoded), Dense(Architecture[-(hiddenLayer + 2)], activation = Activation, kernel_initializer = Initializer)(Decoded)\n",
    "\n",
    "                Encoder, Decoder = Model(Image, Dense(Architecture[-1], activation = Activation)(Dense(Architecture[-2], activation = Activation)(Encoded)), name = \"Encoder\"), Model(decoderInput, Dense(localBound, activation = \"sigmoid\", kernel_initializer = Initializer)(Decoded), name = \"Decoder\")\n",
    "                \n",
    "                # Build main autoencoder\n",
    "                Autoencoder = Model(Image, Decoder(Encoder(Image)))\n",
    "\n",
    "                # Compile the autoencoder\n",
    "                Autoencoder.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = Optimizer)\n",
    "\n",
    "                # Fit the data on the Autoencoder\n",
    "                Autoencoder = Autoencoder.fit(X_train, X_train, epochs = 1, batch_size = 60000, shuffle = True, validation_data = (X_val, X_val), verbose = 0)\n",
    "                Results = Autoencoder.history\n",
    "                Tuner[(Optimizer, Architecture, Initializer, Activation)] = Alpha * (Results[validationAccuracy][0] - Results[validationLoss][0]) + Beta * (Results[Accuracy][0] - Results[Loss][0])\n",
    "\n",
    "Optimizer, Architecture, Initializer, Activation = max(Tuner, key = Tuner.get)\n",
    "optimizerName = Optimizer\n",
    "Optimizer, decoderInput, number_hidden_layers, Tuner = list(Optimizers.keys())[optimizerNames.index(optimizerName)], Input(shape = (Architecture[-1],)), len(Architecture) - 1, {}\n",
    "\n",
    "# Define & build well-tuned encoder & decoder\n",
    "Encoded, Decoded = Dense(Architecture[0], activation = Activation, kernel_initializer = Initializer)(Image), Dense(Architecture[-2], activation = Activation, kernel_initializer = Initializer)(decoderInput)\n",
    "for hiddenLayer in range(1, number_hidden_layers):\n",
    "    Encoded, Decoded = Dense(Architecture[hiddenLayer], activation = Activation, kernel_initializer = Initializer)(Encoded), Dense(Architecture[-(hiddenLayer + 2)], activation = Activation, kernel_initializer = Initializer)(Decoded)\n",
    "\n",
    "Encoder, Decoder = Model(Image, Dense(Architecture[-1], activation = Activation)(Dense(Architecture[-2], activation = Activation)(Encoded)), name = \"Encoder\"), Model(decoderInput, Dense(localBound, activation = \"sigmoid\", kernel_initializer = Initializer)(Decoded), name = \"Decoder\")\n",
    "# Search for good-enough learning rate\n",
    "for learningRate in learningRates:\n",
    "    # Build main autoencoder\n",
    "    Autoencoder = Model(Image, Decoder(Encoder(Image)))\n",
    "\n",
    "    # Compile the autoencoder\n",
    "    Autoencoder.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = Optimizer(learning_rate = learningRate))\n",
    "\n",
    "    # Fit the data on the Autoencoder\n",
    "    Autoencoder = Autoencoder.fit(X_train, X_train, epochs = 1, batch_size = 60000, shuffle = True, validation_data = (X_val, X_val), verbose = 0)\n",
    "    Results = Autoencoder.history\n",
    "    Tuner[learningRate] = Alpha * (Results[validationAccuracy][0] - Results[validationLoss][0]) + Beta * (Results[Accuracy][0] - Results[Loss][0])\n",
    "\n",
    "learningRate = max(Tuner, key = Tuner.get)\n",
    "print(f\"\\nOptimal optimizer, architecture, primary weights initializer method, activation functions for hidden layers and learning rate are {optimizerName}, {Architecture}, {Initializer}, {Activation} and {learningRate}, respectively.\\n\\nTrainig optimal autoencoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Autoencoder\n",
    "Autoencoder is now constructed with chosen values of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build main autoencoder\n",
    "Autoencoder = Model(Image, Decoder(Encoder(Image)))\n",
    "\n",
    "# Compile the autoencoder\n",
    "Autoencoder.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = Optimizer(learning_rate = learningRate))\n",
    "print(Autoencoder.summary())\n",
    "\n",
    "# Saving model\n",
    "Autoencoder.save(\"Autoencoder.keras\")\n",
    "\n",
    "# Fit the data on the Autoencoder\n",
    "# Test set is now the validation set of model!\n",
    "autoencoderHistory = Autoencoder.fit(X_train, X_train, epochs = 10, batch_size = 256, shuffle = True, validation_data = (X_test, X_test), verbose = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Evaluation Plotting\n",
    "Final loss value and accuracy of model's perfomance on train set and test set is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(autoencoderHistory.history[Accuracy])\n",
    "plot(autoencoderHistory.history[validationAccuracy])\n",
    "plot(autoencoderHistory.history[Loss])\n",
    "plot(autoencoderHistory.history[validationLoss])\n",
    "title(Title)\n",
    "ylabel(\"Loss\")\n",
    "xlabel(\"Epochs\")\n",
    "legend([\"Train Accuracy\", \"Test Accuracy\", \"Train Loss\", \"Test Loss\"], loc = \"upper right\")\n",
    "savefig(Title, dpi = 1200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Encoding\n",
    "Datasets get encoded in their corresponding batch sizes (small enough to get better accuracies and also save time!) which are 700, 300 and 256 for train set, test set and validation set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding images\n",
    "X_train, X_test, X_val, Title = Encoder.predict(X_train, batch_size = 700), Encoder.predict(X_test, batch_size = 300), Encoder.predict(X_val, batch_size = 256), \"Elbow Method\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which number of Clusters is optimal?\n",
    "According to following plot, 10 number of clusters is optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which number of clusters should be chosen as best one\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(range(1, 11), [KMeans(algorithm = \"elkan\", init = \"k-means++\", max_iter = 1000, n_clusters = numberCluster, n_init = \"auto\", random_state = 42).fit(X_train).inertia_ for numberCluster in range(1, 11)], marker = \"o\")\n",
    "title(Title)\n",
    "xlabel(\"Number of Clusters\")\n",
    "ylabel(\"Inertia\")\n",
    "savefig(Title, dpi = 1200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means & Mini-Batch K-Means Clustring Algorithms\n",
    "Two algorithms from 'K-Means' family are implemented (by means of 'scikit-learn' library) and selected centroids are plotted (which shows that accuracy is very low; as it'll be calculated)\n",
    "These models once get tuned by 'GridSearchCV' method.\n",
    "Many measures as clustering algorithm evaluation metrics are calculated and printed.\n",
    "According to results for once (illustrated in 'Evaluations' folder), did better in many metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, fit (train) & evaluate four models\n",
    "KAverages = {\"K-Means Clustering Algorithm\": (KMeans, GridSearchCV(KMeans(), param_grid = {\"algorithm\": [\"elkan\", \"lloyd\"], \"init\": [\"k-means++\", \"random\"], \"max_iter\": [100, 200, 500, 1000], \"n_clusters\": [10], \"n_init\": [\"auto\"]}, n_jobs = -1, cv = 5)), \"Mini-Batch K-Means Clustering Algorithm\": (MiniBatchKMeans, GridSearchCV(MiniBatchKMeans(), param_grid = {\"init\": [\"k-means++\", \"random\"], \"max_iter\": [100, 200, 500, 1000], \"n_clusters\": [10], \"n_init\": [\"auto\"]}, n_jobs = -1, cv = 5))}\n",
    "for clustererName in KAverages:\n",
    "    print(f\"\\n{clustererName} Modeling Started!\")\n",
    "    # Using Bayes Search approach for tuning model\n",
    "    Clusterer, Title = KAverages[clustererName], f\"{clustererName} Confusion Matrix\"\n",
    "    model = Clusterer[1]\n",
    "    startTime = time()\n",
    "    # Obtaining best paramteres by using splitted validation data\n",
    "    model.fit(X_val)\n",
    "    Duration = time() - startTime # Calculating how long fitting validation data takes\n",
    "    print(f\"\\nModel's best score is:\\n{model.best_score_}\\n\\nModel's best parameters are:\\n{model.best_params_}\")\n",
    "    model = model.best_estimator_ # = Model(Optimal Parameters)\n",
    "    print(f\"\\n{model}\")\n",
    "    # Evaluation Time!\n",
    "    model.fit(X_train)\n",
    "    Duration, Labels, Centroids, realLabels = time() - Start, model.labels_, model.cluster_centers_, len(unique(y_train))\n",
    "    y_pred, nearestInstances, clusterIndexes, clustered_data_points, numberClusters = model.fit_predict(X_test), pairwise_distances_argmin(Centroids, X_train), [[] for realLabel in range(realLabels)], [[] for realLabel in range(realLabels)], len(unique(Labels))\n",
    "    Trace = [[] for Cluster in range(numberClusters)]\n",
    "    \n",
    "    # Declaring number of datapoints in individual clusters\n",
    "    for Index, Label in enumerate(Labels):\n",
    "        for Number in range(numberClusters):\n",
    "            if Label == Number:\n",
    "                clustered_data_points[Number].append(Index)\n",
    "    \n",
    "    for Cluster in range(numberClusters):\n",
    "        print(f\"No. of items in {Cluster}th cluster: {len(clustered_data_points[Cluster])}\")\n",
    "    print(f\"\\nAll {sum([len(dataPoints) for dataPoints in clustered_data_points])} training data points are clustered.\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Plotting clustered training samples in 3D\n",
    "    for Cluster in range(numberClusters):\n",
    "        Members = (clusterIndexes[Cluster])\n",
    "        Trace[Cluster] = Scatter3d(x = arrX_train[Members, 0], y = arrX_train[Members, 1], z = arrX_train[Members, 2], mode = \"markers\", marker = dict(size = 2, color = Colors[Cluster]), name = f\"Cluster {Cluster}\", hoverinfo = \"text\")\n",
    "        layout = Layout(title = \"3D Scatter Plot\")\n",
    "    iplot(Figure(data = [Trace[0], Trace[1], Trace[2], Trace[3], Trace[4], Trace[5], Trace[6], Trace[7], Trace[8], Trace[9]], layout = layout))\n",
    "\n",
    "    uniqueLabels = unique(y_pred)\n",
    "    # Plotting the clusters\n",
    "    figure(figsize = (8, 8)) \n",
    "    for i in unique_labels:\n",
    "        scatter(arrtestSet[\"label\" == i, 0], arrtestSet[\"label\" == i, 1], label = i)\n",
    "    scatter(Centroids[:, 0], Centroids[:, 1], marker = \"x\", s = 169, linewidths = 3, color = \"k\", zorder = 10) \n",
    "    legend() \n",
    "    show()\n",
    "    # Use sklearn function to calculate the nearest data instance \n",
    "    # for each cluster center. The function returns the indices of \n",
    "    # the nearest instances.\n",
    "    \"\"\"\n",
    "    numberClusters = len(unique(y_pred))\n",
    "    # Plotting selected centroids by implied algorithm\n",
    "    Fig, Axes = subplots(1, numberClusters)\n",
    "    for Centroid in range(numberClusters):\n",
    "        # Plot the images\n",
    "        Axes[Centroid].imshow(arrX_train[nearestInstances[Centroid]], cmap = \"grey\")\n",
    "\n",
    "        # Styling\n",
    "        Axes[Centroid].set_yticks([])\n",
    "        Axes[Centroid].set_xticks([])\n",
    "    title(f\"Centroids\\nProvided by {clustererName}\")\n",
    "    savefig(f\"Centroids by {clustererName}\", dpi = 1200)\n",
    "    show()\n",
    "\n",
    "    # Evaluation metrics\n",
    "    print(f\"Model is trained in {Duration} UTC\\n\\nMeasures:\\nAccuracy: {accuracy_score(y_test, y_pred)}\\nAdjusted Mutual Information (AMI) Score: {round(adjusted_mutual_info_score(y_test, y_pred) * 100, 2)}\\nAdjusted Rand Index (ARI) Score: {round(adjusted_rand_score(y_test, y_pred) * 100, 2)}\\nCalinski Harabasz Score: {calinski_harabasz_score(X_test, y_pred)}\\nCompleteness Score: {completeness_score(y_test, y_pred)}\\nDavies Bouldin Score: {davies_bouldin_score(X_test, y_pred)}\\nHomogeneity Score: {homogeneity_score(y_test, y_pred)}\\nMutual Info Score: {mutual_info_score(y_test, y_pred)}\\nRand Score: {rand_score(y_test, y_pred)}\\nSSE of random data's cluster results: {model.inertia_}\\nSilhouette Score: {silhouette_score(X_test, y_pred, metric = 'euclidean', sample_size = 300)}\\nV Measure: {v_measure_score(y_test, y_pred)}\")\n",
    "    ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred)).plot()\n",
    "    title(Title)\n",
    "    savefig(Title, dpi = 1200)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering Algorithm\n",
    "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering algorithm is implemented in two forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the clustering using DBSCAN\n",
    "print(\"\\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) Clustering Algorithm\")\n",
    "Title = \"DBSCAN Clustering Algorithm Confusion Matrix\"\n",
    "startTime = time()\n",
    "model = DBSCAN(algorithm = \"brute\", eps = 30, metric = \"euclidean\", min_samples = 50)\n",
    "model.fit(X_train)\n",
    "y_pred = model.fit_predict(X_test)\n",
    "Duration = time() - startTime\n",
    "# Evaluation metrics\n",
    "print(f\"Model is trained in {Duration} UTC\\n\\nMeasures:\\nAccuracy: {accuracy_score(y_test, y_pred)}\\nAdjusted Mutual Information (AMI) Score: {round(adjusted_mutual_info_score(y_test, y_pred) * 100, 2)}\\nAdjusted Rand Index (ARI) Score: {round(adjusted_rand_score(y_test, y_pred) * 100, 2)}\\nCompleteness Score: {completeness_score(y_test, y_pred)}\\nHomogeneity Score: {homogeneity_score(y_test, y_pred)}\\nMutual Info Score: {mutual_info_score(y_test, y_pred)}\\nRand Score: {rand_score(y_test, y_pred)}\\nV Measure: {v_measure_score(y_test, y_pred)}\")\n",
    "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred)).plot()\n",
    "title(Title)\n",
    "savefig(Title, dpi = 1200)\n",
    "show()\n",
    "\n",
    "# Silhouette cluster evaluation.\n",
    "model = clusteval(evaluate = \"silhouette\")\n",
    "# In case of using dbindex, it is best to clip the maximum number of clusters to avoid finding local minima.\n",
    "model = clusteval(evaluate = \"dbindex\", max_clust = 10)\n",
    "# Derivative method.\n",
    "model = clusteval(evaluate = \"derivative\")\n",
    "# DBscan method.\n",
    "model = clusteval(cluster = \"dbscan\", params_dbscan = {\"epsres\": 100, \"norm\": True})\n",
    "\n",
    "Results = model.fit(X_train)\n",
    "# Clustering labels\n",
    "print(Results[\"labx\"])\n",
    "\n",
    "model.plot()\n",
    "model.plot_silhouette()\n",
    "model.scatter()\n",
    "model.dendrogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN Clustering Algorithm\n",
    "Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) clustering algorithm is implemented in two forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the clustering using HDBSCAN\n",
    "print(\"\\nHierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) Clustering Algorithm\")\n",
    "Title = \"HDBSCAN Clustering Algorithm Confusion Matrix\"\n",
    "startTime = time()\n",
    "model = HDBSCAN(algorithm = \"best\", alpha = 1.0, approx_min_span_tree = True, cluster_selection_method = \"eom\", gen_min_span_tree = False, leaf_size = 40, metric = \"euclidean\", min_cluster_size = 5, min_samples = 50)\n",
    "model.fit(X_train)\n",
    "y_pred = model.fit_predict(X_test)\n",
    "Duration = time() - startTime\n",
    "# Evaluation metrics\n",
    "print(f\"Model is trained in {Duration} UTC\\nMeasures:\\nAccuracy: {accuracy_score(y_test, y_pred)}\\nAdjusted Mutual Information (AMI) Score: {round(adjusted_mutual_info_score(y_test, y_pred) * 100, 2)}\\nAdjusted Rand Index (ARI) Score: {round(adjusted_rand_score(y_test, y_pred) * 100, 2)}\\nCalinski Harabasz Score: {calinski_harabasz_score(X_test, y_pred)}\\nCompleteness Score: {completeness_score(y_test, y_pred)}\\nDavies Bouldin Score: {davies_bouldin_score(X_test, y_pred)}\\nHomogeneity Score: {homogeneity_score(y_test, y_pred)}\\nMutual Info Score: {mutual_info_score(y_test, y_pred)}\\nRand Score: {rand_score(y_test, y_pred)}\\nSilhouette Score: {silhouette_score(X_test, y_pred, metric = 'euclidean', sample_size = 300)}\\nV Measure: {v_measure_score(y_test, y_pred)}\")\n",
    "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred)).plot()\n",
    "title(Title)\n",
    "savefig(Title, dpi = 1200)\n",
    "show()\n",
    "\n",
    "model = clusteval(cluster = \"hdbscan\")\n",
    "# Evaluate\n",
    "Results = model.fit(X_train)\n",
    "print(Results)\n",
    "Labels = Results[\"labx\"]\n",
    "print(Labels)\n",
    "\n",
    "# Make plot of the evaluation\n",
    "model.plot()\n",
    "# Make scatter plot using the first two coordinates. \n",
    "model.scatter(X_train)\n",
    "\n",
    "# Compute the enrichment of the cluster labels with the dataframe df\n",
    "print(enrichment(dfX_train, Labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Accuracy of HDBSCAN clustering algorithm on encoded images was the best (approximately equals 0.01). Evaluation metrics got too much small values and near to each other while autoencoder was trained with 'Nadam' optimizer, (500, 100, 30) architecture and with linear activation functions, as a study case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "Main challenge was to fine tune autoencoder automatically (especially for different architectures) sync encoded images for getting clustered with four different clustering algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
